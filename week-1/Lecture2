# 批次梯度下降太慢，每次更新参数前，每一步的梯度下降都要计算损失函数的总和。当数据集很大时，成本高，速度慢。如果数据集小，可以用。
# 大数据集一般采用随机梯度下降，但这种方式不能完全收敛。
# 线性回归没有局部最优。虽然梯度下降容易受到局部最优的影响，但是线性回归的优化问题只有一个全局，而没有其他局部最优，因此线性回归的梯度下降总是收敛到全局最优。
# 正态方程，只适合线性回归。只需一步，即可达到全局最优。
# 预测值是连续的，是回归；离散的，则为分类。
# 随机梯度下降（增量梯度下降）：每次遇到一个训练数据时，只根据单个训练数据的误差梯度更新参数。
  虽然梯度下降速度更快，但是可能永远无法收敛到最优，权重参数会在最小值附近震荡。实际中，这些值以及效果很好了。数据集很大时，用这个。
